<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lee Kezar</title>
    <link rel="stylesheet" type="text/css" href="./styles.css">
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
    <script src="https://code.jquery.com/ui/1.12.1/jquery-ui.min.js"></script>
    <script src="app.js"></script>
</head>
<body>

    <header>
        <div class="container">
            <span class="left-align">
                <p>
                    <h1>Hi, I'm <span class="highlight-noexpand" style="padding-left:8px; padding-right:8px;">Lee Kezar</span></h1>
                    <h3>PhD Candidate, University of Southern California</h3>

                    I'm a Graduate Research Assistant in the <span class="highlight"><a href="http://glamor.rocks/" target="_blank">GLAMOR Lab</a></span> Lab under 
                    <span class="highlight"><a href="https://jessethomason.com/" target="_blank">Dr. Jesse Thomason</a></span>.<br>

                    I'm also a Visiting Scholar in the <span class="highlight"><a href="https://sites.bu.edu/lexlab/" target="_blank">LexLab</a></span> under 
                    <span class="highlight"><a href="http://www.naomicaselli.com/" target="_blank">Dr. Naomi Caselli</a></span> at Boston University's 
                    <span class="highlight"><a href="https://sites.bu.edu/deafcenter/" target="_blank">Deaf Center</a></span>.
                </p>

                <div id="progressbar"><div class="progress-label"></div></div>
                <p>âœ¨ I'm looking for industry-based internships (for May 2024) and full-time roles (starting January 2025)!</p>
            </span>
            <div class="line"></div>
            <span class="right-align">
                <img src="images/selfie.jpeg" class="selfie">
            </span>
        </div>
    </header>

    <main>
        <div>
            <header class="container banner">
                <span class="left-align">
                    <h2>My Research</h2>
                </span>
                <div class="line"></div>
                <span class="right-align">
                    <span class="expandable highlight" id="expand">Expand All</span>
                    <span class="expandable highlight" id="collapse">Collapse All</span>
                </span>
            </header>

                <section style="margin-top:0;">
                    <p>
                        As language technologies like ChatGPT gain popularity, deaf people who predominantly use a sign language to communicate are increasingly left behind. 
                        To help make these technologies more accessible, I study ways to make models that can <span class="expandable highlight" id="understanding">understand sign language</span>. 
                        My current work focuses on <span class="expandable highlight" id="semantics">lexical semantics</span>, leveraging <span class="expandable highlight" id="knowledgebase">linguistic knowledge bases</span> to improve model 
                        <span class="expandable highlight" id="perception">perception</span>, <span class="expandable highlight" id="recognition">recognition</span>, and <span class="expandable highlight" id="comprehension">comprehension</span>.
                    </p>
                </section>

                <section class="expansion" id="understanding">
                    <p>
                        <span class="highlight-noexpand">Understanding Sign Language</span><br>
                        Understanding any sign language requires <span class="expandable highlight" id="perception">perceiving</span> visual features like the location and configuration of the hands, <span class="expandable highlight" id="recognition">recognizing</span> what those features signify, and 
                        <span class="expandable highlight" id="comprehension">comprehending</span> the intended meaning through inference. 
                        To satisfy these requirements, I adopt a <span class="expandable highlight" id="ling">linguistic</span> approach. 
                        My work presents the argument that a model which can recognize phonological features not only recognizes isolated signs more accurately, but also enables it to approximate the meaning of signs it has never seen before!
                    </p>
                    <section class="expansion" id="perception">
                        <p>
                            <span class="highlight-noexpand">Perception</span><br>
                            Understanding sign language begins with perceiving many subtle visual features called <span class="expandable highlight" id="phonemes">phonemes</span>, for example the flexion of each finger or the path that the hands move along. 
                            Our EACL paper "Improving Sign Recognition with Phonology" <span class="highlight"><a href="https://arxiv.org/abs/2302.05759" target="_blank">Paper</a></span> shows that models miss many of these features, leading to poorer performance in higher-level tasks like recognizing signs. 
                            To improve models' phonological perception, a follow-up work "Exploring Strategies for Modeling Sign Language Phonology" <span class="highlight"><a href="https://arxiv.org/abs/2310.00195" target="_blank">Paper</a></span> provides a method which recognizes 16 types of phonological features with 75%-90% accuracy using a technique called curriculum learning.
                        </p>
                    </section>
            
                    <section class="expansion" id="recognition">
                        <p>
                            <span class="highlight-noexpand">Recognition</span><br>
                            Recognizing which sign(s) are present in a video is a critical step towards automatic sign language translation and requires distinguishing many visually-similar signs. 
                            This is a difficult task for rare and modified signs, which are unlikely to have been seen in training.
                            Our paper, "Improving Sign Recognition with Phonology" <span class="highlight"><a href="https://arxiv.org/abs/2302.05759" target="_blank">Paper</a></span> showed that models are better at distinguishing signs when they can also recognize the pieces of those signs, i.e. <span class="expandable highlight" id="phonemes">phonemes</span>. 
                            This result was replicated in "The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes" <span class="highlight"><a href="https://arxiv.org/abs/2302.05759" target="_blank">Paper</a></span>, adding that rare signs benefit the most from phoneme recognition.                       
                         </p>
                    </section>

                    <section class="expansion" id="comprehension">
                        <p>
                            <span class="highlight-noexpand">Comprehension</span><br>
                            One powerful benefit of modeling sign language <span class="expandable highlight" id="phonemes">phonology</span> is the ability to apply inference to the phonemes after they are recognized. 
                            Specifically, by leveraging systematic relationships between form and meaning, we may be able to infer the meaning of a sign even if the model has never seen it before! 
                            Ongoing work explores this possibility, which we intend to publish in 2024.
                        </p>
                </section>
            </section>

            <section class="expansion" id="ling">
                <p>
                    <span class="highlight-noexpand">Sign Language Linguistics</span><br>
                    Sign languages are like spoken languages in almost every way. 
                    Each sign language has its own vocabulary and grammar, and they are independent of any spoken language used in their region. 
                    They have a complex structure at all levels, such as <span class="expandable highlight" id="phonemes">phonology</span> and <span class="expandable highlight" id="phonemes">semantics</span>. 
                    My work leverages the relationship between phonology and semantics, also known as systematicity, to help machine learning models understand signs they haven't seen before.
                </p>

                <section class="expansion" id="phonology">
                    <p>
                        <span class="highlight-noexpand">Phonemes</span><br>
                        Phonemes are the smallest units of language. 
                        In sign languages, phonemes fall into one of several categories, such as hand configuration, movement, and location. 
                        My work loosely falls under Brentari's Prosodic Model, which emphasizes the simultaneity and interdependency of phonemes.
                    </p>
                </section> 

                <section class="expansion" id="semantics">
                    <p>
                        <span class="highlight-noexpand">Lexical Semantics</span><br>
                        Many times, the meaning of a sign has a direct relationship to its form. 
                        This relationship, called systematicity, provides valuable scaffolding to learn signs more efficiently and can enable understanding even when the sign has never been seen before. 
                        Currently, my team is working on a large knowledge graph of ASL signs' form and meaning, which we will be releasing in Fall 2024. 
                        We hope this resource will be used in creative ways to computationally model ASL systematicity.                       
                    </p>
                </section> 

                <section class="expansion" id="knowledgebase">
                    <p>
                        <span class="highlight-noexpand">Linguistic Knowledge Bases</span><br>
                        Structured, accurate knowledge about the structure of signs can be a powerful addition to current models for sign language. 
                        For example, training models to simultaneously recognize phonological features alongside sign identifiers improves their accuracy at both tasks. 
                        My work relies on knowledge bases like <span class="highlight"><a href="https://asl-lex.org/" target="_blank">ASL-LEX 2.0</a></span>, a dataset of linguistic and cognitive facts related to signs, and <span class="highlight"><a href="https://hci.stanford.edu/publications/2016/ethan/empath-chi-2016.pdf" target="_blank">Empath</a></span>, a dataset of semantic categories for many English words. 
                        We've also released a dataset of our own, the Sem-Lex Benchmark <span class="highlight"><a href="https://arxiv.org/abs/2302.05759" target="_blank">Paper</a></span>, containing over 91,000 isolated sign videos representing a vocabulary of 3,100 signs (the largest of its kind!).                        
                    </p>
                </section> 
            </section>
        </div>

        <div>
            <header class="container banner">
                <span class="left-align"><h2>Resources</h2></span>
            </header>
            <div>
                <section>
                    <p>
                        Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi iaculis nisl eu sapien ultrices, a mattis est pharetra. Phasellus eget tellus enim. Aenean varius ultrices porta. Sed sed cursus arcu. Integer ut cursus purus, non tempus diam. Aliquam laoreet lacinia justo ut vehicula. Aliquam gravida, leo nec posuere ullamcorper, ex magna scelerisque nulla, at posuere ante libero a nisi. Curabitur elit nisi, molestie vitae sem eget, aliquet posuere nibh. Cras augue lacus, accumsan pretium bibendum sit amet, pellentesque vel orci. Phasellus porta tellus vitae vulputate hendrerit. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Suspendisse imperdiet mauris in egestas ornare. Nam nulla sem, pulvinar maximus dolor sed, ullamcorper egestas arcu. Suspendisse id commodo ligula. Curabitur metus massa, tempor ut facilisis volutpat, malesuada at orci.

                        Etiam viverra iaculis ligula sit amet dapibus. Duis urna dui, ornare in elit et, ultricies vestibulum ante. Aenean quis nisl nec leo euismod egestas. Etiam nec feugiat ex. In at sem ac dolor laoreet laoreet quis sit amet enim. Integer ornare malesuada vulputate. Donec sit amet nibh id risus finibus efficitur ut ornare enim. Donec vehicula arcu id justo suscipit blandit. Morbi varius posuere est at rhoncus. Quisque dapibus quis nibh a porta. Sed a lectus id mauris scelerisque bibendum. Nullam non nisi id massa mattis pellentesque. Aenean sit amet nisl vitae ante imperdiet tempus. Curabitur elementum purus vitae dictum elementum. Integer accumsan lacus dui. Nullam et pharetra sem.

                        Duis bibendum iaculis enim quis porttitor. Nullam tempus ullamcorper vehicula. Proin tristique sed leo non faucibus. Mauris scelerisque elit et neque convallis, vel pretium sapien euismod. Donec euismod justo dui, quis vulputate velit lacinia quis. Sed molestie maximus velit, in sodales tortor sodales id. Curabitur congue eu tellus ut fringilla.

                        Phasellus urna odio, consequat a velit ut, laoreet vestibulum sapien. Morbi suscipit ac arcu pretium pretium. Aliquam auctor turpis sed euismod volutpat. Vivamus ac tortor vitae metus porttitor lobortis. Fusce id aliquet sapien. Fusce in porttitor magna, eget blandit mauris. Proin porta diam at nisi convallis mattis. Maecenas eleifend leo et metus ullamcorper dapibus. Mauris eget vestibulum massa.

                        Nulla luctus non odio id convallis. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vivamus vestibulum sit amet justo vel viverra. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Donec eu nibh nunc. Vivamus ac lobortis justo. Sed eget tempor sapien. Sed mollis felis laoreet dictum auctor. Vivamus finibus, felis vel consectetur lobortis, risus ante posuere metus, vitae tempus dui leo vitae sapien. Praesent quis turpis congue, consequat metus vitae, sagittis lorem. Vestibulum fringilla, nisi sed aliquet ullamcorper, sem nisi efficitur mi, at ultricies tellus nisl et massa. Morbi bibendum consectetur felis. Sed in risus non nisl consequat ullamcorper nec at ante. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed sed faucibus erat, a dignissim nunc. Morbi nec tincidunt nibh.
                    </p>
                </section>
            </div>
        </div>
    </main>

</body>
</html>