<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lee Kezar</title>
  <link rel="stylesheet" type="text/css" href="./styles.css" />

  <!-- Dependencies -->
  <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  <script src="https://code.jquery.com/ui/1.12.1/jquery-ui.min.js"></script>

  <!-- ✅ Working BibTeX parser (global variable bibtexParse) -->
  <script src="https://cdn.jsdelivr.net/npm/bibtex-parse-js@0.0.26/bibtexParse.js"></script>

  <!-- Core scripts -->
  <script src="app.js"></script>
</head>

<body>
  <header>
    <div class="container">
      <span class="left-align">
        <p>
          <h1>
            Hi, I'm
            <span class="highlight-noexpand" style="padding-left:8px; padding-right:8px;">Dr. Lee Kezar</span>
          </h1>
          I develop AI language models for American Sign Language using linguistically-informed techniques.
          <br /><br />
          I'm a Postdoctoral Research Associate in the
          <span class="highlight"><a href="https://sites.google.com/gallaudet.edu/action-brain-lab" target="_blank">Action and Brain Lab</a></span>
          under
          <span class="highlight"><a href="https://www.lornaquandt.com/" target="_blank">Dr. Lorna Quandt</a></span>,
          where I focus on building AI-powered learning tools for deaf and hard-of-hearing students in STEM classrooms.
          <br /><br />
          I received my Ph.D. in Computer Science from the University of Southern California in 2025, and my B.S. in Computer Science from Rhodes College in 2019.
          <span class="highlight"><a href="/pdfs/LeeKezarCV.pdf" target="_blank">Click here</a></span> for my curriculum vitae.
        </p>
      </span>
      <div style="flex-grow:1">
        <img src="images/selfie.jpeg" class="selfie" />
      </div>
    </div>
  </header>

  <main>
    <!-- ===== Research Agenda ===== -->
    <header class="container banner">
      <span class="left-align">
        <h2>Research Agenda</h2>
      </span>
      <div class="line"></div>
      <span class="right-align">
        <span class="expandable highlight" id="expand_concepts">Expand All</span>
        <span class="expandable highlight" id="collapse_concepts">Collapse All</span>
      </span>
    </header>

    <section style="margin-top:0;">
      <p>
        As language technologies like ChatGPT gain popularity, users who are most comfortable communicating with a sign language are increasingly left behind.
        To help make these technologies more accessible, I use
        <span class="super expandable concept highlight" id="ling">sign language linguistics</span>
        to make models that can
        <span class="super expandable concept highlight" id="understanding">understand sign language</span>.
        My current work focuses on
        <span class="expandable concept highlight" id="semantics">lexical semantics</span>,
        leveraging
        <span class="expandable concept highlight" id="knowledgebase">linguistic knowledge bases</span>
        to improve model
        <span class="expandable concept highlight" id="perception">perception</span>,
        <span class="expandable concept highlight" id="recognition">recognition</span>,
        and
        <span class="expandable concept highlight" id="comprehension">comprehension</span>.
      </p>
    </section>

    <section class="super concept expansion" id="understanding">
      <p>
        <span class="highlight-noexpand">Understanding Sign Language</span><br />
        Understanding any sign language requires
        <span class="expandable concept highlight" id="perception">perceiving</span>
        visual features like the location and configuration of the hands,
        <span class="expandable concept highlight" id="recognition">recognizing</span>
        what those features signify, and
        <span class="expandable concept highlight" id="comprehension">comprehending</span>
        the intended meaning through inference.
        To satisfy these requirements, I adopt a
        <span class="super expandable concept highlight" id="ling">linguistic</span> approach.
        My work presents the argument that a model which can recognize phonological features not only recognizes isolated signs more accurately, but also enables it to approximate the meaning of signs it has never seen before!
      </p>

      <section class="concept expansion" id="perception">
        <p>
          <span class="highlight-noexpand">Perception</span><br />
          Understanding sign language begins with perceiving subtle visual features called
          <span class="expandable concept highlight" id="phonology">phonemes</span>,
          such as finger flexion or motion path. Our EACL paper
          <span class="expandable paper highlight" id="eacl">Improving Sign Recognition with Phonology</span>
          shows that models miss many of these features, leading to weaker recognition. To improve models' phonological perception, the follow-up work
          <span class="expandable paper highlight" id="esann">Exploring Strategies for Modeling Sign Language Phonology</span>
          provides a method that recognizes 16 types of phonological features with 75–90% accuracy via curriculum learning.
        </p>
      </section>

      <section class="concept expansion" id="recognition">
        <p>
          <span class="highlight-noexpand">Recognition</span><br />
          Recognizing which sign(s) are present in a video is key to automatic translation and requires distinguishing many visually similar signs.
          This is difficult for rare and modified signs, which models seldom see in training.
          Our paper
          <span class="expandable paper highlight" id="eacl">Improving Sign Recognition with Phonology</span>
          showed that models perform better when they also recognize the pieces of signs, i.e.
          <span class="expandable concept highlight" id="phonology">phonemes</span>.
          This was replicated in
          <span class="expandable paper highlight" id="assets">The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes</span>,
          showing that rare signs benefit most from phoneme recognition.
        </p>
      </section>

      <section class="concept expansion" id="comprehension">
        <p>
          <span class="highlight-noexpand">Comprehension</span><br />
          Modeling sign language
          <span class="expandable concept highlight" id="phonology">phonology</span>
          allows inference over recognized phonemes. Leveraging systematic relationships between form and meaning may let us infer the meaning of unseen signs. Ongoing work explores this, aiming for publication in 2024.
        </p>
      </section>
    </section>

    <section class="super concept expansion" id="ling">
      <p>
        <span class="highlight-noexpand">Sign Language Linguistics</span><br />
        Sign languages are comparable to spoken languages in almost every way. Each has its own vocabulary and grammar, independent of any spoken language. They have structure at all levels, such as
        <span class="expandable concept highlight" id="phonology">phonology</span>
        and
        <span class="expandable concept highlight" id="semantics">semantics</span>.
        My work leverages the relationship between phonology and semantics—systematicity—to help models understand unseen signs.
        <span class="super expandable concept highlight" id="understanding">understand sign language</span>
      </p>

      <section class="concept expansion" id="phonology">
        <p>
          <span class="highlight-noexpand">Phonemes</span><br />
          Phonemes are the smallest linguistic units. In sign languages, they include hand configuration, movement, and location.
          My work follows Brentari’s Prosodic Model, emphasizing simultaneity and interdependence of phonemes.
        </p>
      </section>

      <section class="concept expansion" id="semantics">
        <p>
          <span class="highlight-noexpand">Lexical Semantics</span><br />
          Many signs have a direct form–meaning relationship, or systematicity. This provides scaffolding for learning and inference over unseen signs.
          My team is developing a large ASL form–meaning knowledge graph, releasing Fall 2024, to model systematicity computationally.
        </p>
      </section>

      <section class="concept expansion" id="knowledgebase">
        <p>
          <span class="highlight-noexpand">Linguistic Knowledge Bases</span><br />
          Structured knowledge of sign structure enhances sign language models. Training models to recognize phonological features alongside sign identifiers improves both tasks.
          I rely on knowledge bases like
          <span class="highlight"><a href="https://asl-lex.org/" target="_blank">ASL-LEX 2.0</a></span>
          and
          <span class="highlight"><a href="https://hci.stanford.edu/publications/2016/ethan/empath-chi-2016.pdf" target="_blank">Empath</a></span>,
          and have released
          <span class="expandable paper highlight" id="assets">The Sem-Lex Benchmark</span>,
          containing 91 000 isolated sign videos for 3 100 signs.
        </p>
      </section>
    </section>

        <!-- ===== Publications ===== -->
    <header class="container banner">
      <span class="left-align"><h2>Publications</h2></span>
      <div class="line"></div>
      <span class="right-align">
        <span class="expandable highlight" id="expand_papers">Expand All</span>
        <span class="expandable highlight" id="collapse_papers">Collapse All</span>
      </span>
    </header>

    <div id="publications-container"></div>
    <noscript><p><em>Enable JavaScript to view dynamically generated publications.</em></p></noscript>

    <!-- ✅ Load bibtex parser right before usage -->
    <script src="https://cdn.jsdelivr.net/npm/bibtex-parse-js@0.0.26/bibtexParse.js"></script>

    <!-- ✅ Now load generator -->
    <script src="generatePubs.js"></script>
  </main>
</body>
</html>
